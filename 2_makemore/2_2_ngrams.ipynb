{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "\n",
    "1. Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n",
    "It did better (as expected). Table description:\n",
    "- training loss,\n",
    "- no regularisation/smoothing,\n",
    "- 200 epochs for 2/3-gram models,\n",
    "- 600 epochs for 4-gram (probably needs better hyperparameters).\n",
    "\n",
    "| model type    | bigram loss | trigram loss | 4-gram loss  |\n",
    "|---------------|-------------|--------------|--------------|\n",
    "| counting      |       2.454 |        1.942 |        1.471 |\n",
    "| backprop (nn) |       2.460 |        2.029 |        1.780 |\n",
    "\n",
    "2. Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "200 epochs training \n",
    "\n",
    "| model      | train loss | val loss | test loss |\n",
    "|------------|------------|----------|-----------|\n",
    "| bigram nn  |      2.459 |    2.464 |     2.466 |\n",
    "| trigram nn |      1.992 |    2.222 |     2.237 |\n",
    "\n",
    "For bigram model train loss is a little bit better than val and test losses.</br>\n",
    "For trigram model the difference is significant - val and test loss ~10% higher than train loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fpath = './data/names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(data_fpath, 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min len: 2; max len: 15\n"
     ]
    }
   ],
   "source": [
    "word_lens = [len(word) for word in words]\n",
    "print(f'min len: {min(word_lens)}; max len: {max(word_lens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_TOK = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3  # this indicates the n in n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '.', '.'), 6763),\n",
       " (('a', '.', '.'), 6640),\n",
       " (('.', '.', 'a'), 4410),\n",
       " (('e', '.', '.'), 3983),\n",
       " (('.', '.', 'k'), 2963),\n",
       " (('.', '.', 'm'), 2538),\n",
       " (('i', '.', '.'), 2489),\n",
       " (('.', '.', 'j'), 2422),\n",
       " (('h', '.', '.'), 2409),\n",
       " (('.', '.', 's'), 2055)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_dict = {}\n",
    "for word in words:\n",
    "    chars = [SEP_TOK]*(n-1) + list(word) + [SEP_TOK]*(n-1)\n",
    "    ngram_chars = [chars[i:] for i in range(n)]\n",
    "    for ngram in zip(*ngram_chars):\n",
    "        ngrams_dict[ngram] = ngrams_dict.get(ngram, 0) + 1\n",
    "ngrams_dict = sorted(ngrams_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "ngrams_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [SEP_TOK] + sorted(list(set(''.join(words))))\n",
    "stoi = {s: i for i, s in enumerate(vocab)}\n",
    "itos = {i: s for i, s in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram model as an array with counts\n",
    "using generalized version for ngrams given arbitrary n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/32033 [00:00<01:03, 505.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32033/32033 [01:02<00:00, 513.36it/s]\n"
     ]
    }
   ],
   "source": [
    "N = torch.zeros([len(vocab) for _ in range(n)], dtype=torch.int32)\n",
    "for word in tqdm(words):\n",
    "    chars = [SEP_TOK]*(n-1) + list(word) + [SEP_TOK]*(n-1)\n",
    "    ngram_chars = [chars[i:] for i in range(n)]\n",
    "    for ngram in zip(*ngram_chars):\n",
    "        ixs = tuple(stoi[ch] for ch in ngram)\n",
    "        N[ixs] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_count = 0 # smooths the probabilities\n",
    "P = (N+base_count).float()\n",
    "P = P / P.sum(axis=(n-1), keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juniba\n",
      "jakasir\n",
      "presar\n",
      "adria\n",
      "jira\n",
      "tolomas\n",
      "ter\n",
      "kalania\n",
      "yanilena\n",
      "jededaileti\n",
      "tayse\n",
      "siely\n",
      "artez\n",
      "noud\n",
      "than\n",
      "demmerceyn\n",
      "lena\n",
      "jaylie\n",
      "reanae\n",
      "ocely\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "n_samples = 20\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(n_samples):\n",
    "    ixs = deque([stoi[SEP_TOK]] * (n-1))\n",
    "    out = []\n",
    "    while True:\n",
    "        prob_distr = P[tuple(ixs)]\n",
    "        ix = torch.multinomial(prob_distr, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == stoi[SEP_TOK]:\n",
    "            break\n",
    "        ixs.popleft()\n",
    "        ixs.append(ix)\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 138/32033 [00:00<00:47, 672.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32033/32033 [00:46<00:00, 692.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-429832.4062)\n",
      "nll=tensor(429832.4062)\n",
      "nll/count=tensor(1.4710)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for word in tqdm(words, 'Evaluating'):\n",
    "    chars = [SEP_TOK]*(n-1) + list(word) + [SEP_TOK]*(n-1)\n",
    "    ngram_chars = [chars[i:] for i in range(n)]\n",
    "    for ngram in zip(*ngram_chars):\n",
    "        ixs = tuple(stoi[ch] for ch in ngram)\n",
    "        prob = P[ixs]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        count += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/count=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram model as neural net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 3-gram samples: 100%|██████████| 32033/32033 [00:01<00:00, 16626.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 208143\n",
      "Number of validation examples: 26018\n",
      "Number of test examples: 26018\n"
     ]
    }
   ],
   "source": [
    "# creating the training set of bigrams\n",
    "xs, ys = [], []\n",
    "for word in tqdm(words, f'Creating {n}-gram samples'):\n",
    "    chars = [SEP_TOK]*(n-1) + list(word) + [SEP_TOK]*(n-1)\n",
    "    ngram_chars = [chars[i:] for i in range(n)]\n",
    "    for ngram in zip(*ngram_chars):\n",
    "        ixs = [stoi[ch] for ch in ngram]\n",
    "        xs.append(ixs[:-1])\n",
    "        ys.append(ixs[-1])\n",
    "\n",
    "train_split_end = int(0.8 * len(xs))\n",
    "val_split_end = int(0.9 * len(xs))\n",
    "\n",
    "xs_train = torch.tensor(xs[:train_split_end])\n",
    "ys_train = torch.tensor(ys[:train_split_end])\n",
    "\n",
    "xs_val = torch.tensor(xs[train_split_end:val_split_end])\n",
    "ys_val = torch.tensor(ys[train_split_end:val_split_end])\n",
    "\n",
    "xs_test = torch.tensor(xs[val_split_end:])\n",
    "ys_test = torch.tensor(ys[val_split_end:])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)    \n",
    "\n",
    "print(f'Number of training examples: {xs_train.shape[0]}')\n",
    "print(f'Number of validation examples: {xs_val.shape[0]}')\n",
    "print(f'Number of test examples: {xs_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(xs, ys, W, weight_decay=1e-4):\n",
    "    logits = W[[x for x in xs.T]]\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = average negative log likelihood\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean() + weight_decay*(W**2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the \"model\"\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(tuple(len(vocab) for _ in range(n)), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10th epoch, tr_loss=2.644; val_loss=2.855; test_loss=2.868\n",
      " 20th epoch, tr_loss=2.419; val_loss=2.652; test_loss=2.662\n",
      " 30th epoch, tr_loss=2.310; val_loss=2.553; test_loss=2.562\n",
      " 40th epoch, tr_loss=2.243; val_loss=2.488; test_loss=2.498\n",
      " 50th epoch, tr_loss=2.195; val_loss=2.441; test_loss=2.451\n",
      " 60th epoch, tr_loss=2.159; val_loss=2.405; test_loss=2.415\n",
      " 70th epoch, tr_loss=2.131; val_loss=2.376; test_loss=2.387\n",
      " 80th epoch, tr_loss=2.108; val_loss=2.352; test_loss=2.363\n",
      " 90th epoch, tr_loss=2.089; val_loss=2.331; test_loss=2.343\n",
      "100th epoch, tr_loss=2.074; val_loss=2.314; test_loss=2.326\n",
      "110th epoch, tr_loss=2.060; val_loss=2.299; test_loss=2.312\n",
      "120th epoch, tr_loss=2.049; val_loss=2.286; test_loss=2.299\n",
      "130th epoch, tr_loss=2.039; val_loss=2.275; test_loss=2.288\n",
      "140th epoch, tr_loss=2.030; val_loss=2.265; test_loss=2.278\n",
      "150th epoch, tr_loss=2.022; val_loss=2.256; test_loss=2.270\n",
      "160th epoch, tr_loss=2.015; val_loss=2.248; test_loss=2.262\n",
      "170th epoch, tr_loss=2.008; val_loss=2.240; test_loss=2.255\n",
      "180th epoch, tr_loss=2.002; val_loss=2.234; test_loss=2.248\n",
      "190th epoch, tr_loss=1.997; val_loss=2.227; test_loss=2.242\n",
      "200th epoch, tr_loss=1.992; val_loss=2.222; test_loss=2.237\n"
     ]
    }
   ],
   "source": [
    "lr = 100\n",
    "for ep in range(200):\n",
    "    # forward pass\n",
    "    tr_loss = calc_loss(xs_train, ys_train, W, 0)\n",
    "\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    tr_loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -lr * W.grad\n",
    "\n",
    "    if ep % 10 == 9:\n",
    "        tr_loss = calc_loss(xs_train, ys_train, W).item()\n",
    "        val_loss = calc_loss(xs_val, ys_val, W).item()\n",
    "        test_loss = calc_loss(xs_test, ys_test, W).item() \n",
    "        print(f'{ep+1:>3}th epoch, {tr_loss=:.3f}; {val_loss=:.3f}; {test_loss=:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junjdedianaqidouxutnypaxnuq\n",
      "jimrltozsogjatqzvugignaduwjbuldvhajzdbiminrwimpadsvzywcfxvbryn\n",
      "farmumtkyf\n",
      "demmerponnsleigh\n",
      "ani\n",
      "cora\n",
      "yaehocpkqjyked\n",
      "webdmeiibwyaftwtiansnhspoluwaspphfdgosfmxtpqcixz\n",
      "repahfmtydt\n",
      "jayrslu\n",
      "isa\n",
      "dyfj\n",
      "mjluuj\n",
      "mahvupwyilpvhecgiagr\n",
      "jenhwvdxtta\n",
      "malyn\n",
      "brey\n",
      "aui\n",
      "lavlpocq\n",
      "themilana\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "n_samples = 20\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(n_samples):\n",
    "    ixs = deque([stoi[SEP_TOK]] * (n-1))\n",
    "    out = []\n",
    "    while True:\n",
    "        logits = W[tuple(ixs)]\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum()\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == stoi[SEP_TOK]:\n",
    "            break\n",
    "        ixs.popleft()\n",
    "        ixs.append(ix)\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
