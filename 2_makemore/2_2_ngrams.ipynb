{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "\n",
    "1. Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n",
    "It did better (as expected). Table description:\n",
    "- training loss,\n",
    "- no regularisation/smoothing,\n",
    "- 200 epochs for 2/3-gram models,\n",
    "- 600 epochs for 4-gram (probably needs better hyperparameters).\n",
    "\n",
    "| model type    | bigram loss | trigram loss | 4-gram loss  |\n",
    "|---------------|-------------|--------------|--------------|\n",
    "| counting      |       2.454 |        1.942 |        1.471 |\n",
    "| backprop (nn) |       2.460 |        2.029 |        1.780 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fpath = './data/names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(data_fpath, 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min len: 2; max len: 15\n"
     ]
    }
   ],
   "source": [
    "word_lens = [len(word) for word in words]\n",
    "print(f'min len: {min(word_lens)}; max len: {max(word_lens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram model as an array with counts\n",
    "using generalized version for ngrams given arbitrary n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_TOK = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '.', '.', '.'), 6763),\n",
       " (('a', '.', '.', '.'), 6640),\n",
       " (('.', '.', '.', 'a'), 4410),\n",
       " (('e', '.', '.', '.'), 3983),\n",
       " (('.', '.', '.', 'k'), 2963),\n",
       " (('.', '.', '.', 'm'), 2538),\n",
       " (('i', '.', '.', '.'), 2489),\n",
       " (('.', '.', '.', 'j'), 2422),\n",
       " (('h', '.', '.', '.'), 2409),\n",
       " (('.', '.', '.', 's'), 2055)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_dict = {}\n",
    "for word in words:\n",
    "    chars = [SEP_TOK]*(n-1) + list(word) + [SEP_TOK]*(n-1)\n",
    "    ngram_chars = [chars[i:] for i in range(n)]\n",
    "    for ngram in zip(*ngram_chars):\n",
    "        ngrams_dict[ngram] = ngrams_dict.get(ngram, 0) + 1\n",
    "ngrams_dict = sorted(ngrams_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "ngrams_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [SEP_TOK] + sorted(list(set(''.join(words))))\n",
    "stoi = {s: i for i, s in enumerate(vocab)}\n",
    "itos = {i: s for i, s in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/32033 [00:00<01:03, 505.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32033/32033 [01:02<00:00, 513.36it/s]\n"
     ]
    }
   ],
   "source": [
    "N = torch.zeros([len(vocab) for _ in range(n)], dtype=torch.int32)\n",
    "for word in tqdm(words):\n",
    "    chars = [SEP_TOK]*(n-1) + list(word) + [SEP_TOK]*(n-1)\n",
    "    ngram_chars = [chars[i:] for i in range(n)]\n",
    "    for ngram in zip(*ngram_chars):\n",
    "        ixs = tuple(stoi[ch] for ch in ngram)\n",
    "        N[ixs] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_count = 0 # smooths the probabilities\n",
    "P = (N+base_count).float()\n",
    "P = P / P.sum(axis=(n-1), keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juniba\n",
      "jakasir\n",
      "presar\n",
      "adria\n",
      "jira\n",
      "tolomas\n",
      "ter\n",
      "kalania\n",
      "yanilena\n",
      "jededaileti\n",
      "tayse\n",
      "siely\n",
      "artez\n",
      "noud\n",
      "than\n",
      "demmerceyn\n",
      "lena\n",
      "jaylie\n",
      "reanae\n",
      "ocely\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "n_samples = 20\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(n_samples):\n",
    "    ixs = deque([stoi[SEP_TOK]] * (n-1))\n",
    "    out = []\n",
    "    while True:\n",
    "        prob_distr = P[tuple(ixs)]\n",
    "        ix = torch.multinomial(prob_distr, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == stoi[SEP_TOK]:\n",
    "            break\n",
    "        ixs.popleft()\n",
    "        ixs.append(ix)\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 138/32033 [00:00<00:47, 672.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32033/32033 [00:46<00:00, 692.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-429832.4062)\n",
      "nll=tensor(429832.4062)\n",
      "nll/count=tensor(1.4710)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for word in tqdm(words, 'Evaluating'):\n",
    "    chars = [SEP_TOK]*(n-1) + list(word) + [SEP_TOK]*(n-1)\n",
    "    ngram_chars = [chars[i:] for i in range(n)]\n",
    "    for ngram in zip(*ngram_chars):\n",
    "        ixs = tuple(stoi[ch] for ch in ngram)\n",
    "        prob = P[ixs]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        count += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/count=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram model as neural net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 4-gram samples:   0%|          | 0/32033 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 4-gram samples: 100%|██████████| 32033/32033 [00:03<00:00, 9515.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 292212\n"
     ]
    }
   ],
   "source": [
    "# creating the training set of bigrams\n",
    "xs, ys = [], []\n",
    "for word in tqdm(words, f'Creating {n}-gram samples'):\n",
    "    chars = [SEP_TOK]*(n-1) + list(word) + [SEP_TOK]*(n-1)\n",
    "    ngram_chars = [chars[i:] for i in range(n)]\n",
    "    for ngram in zip(*ngram_chars):\n",
    "        ixs = [stoi[ch] for ch in ngram]\n",
    "        xs.append(ixs[:-1])\n",
    "        ys.append(ixs[-1])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(f'Number of training examples: {xs.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(xs, ys, W, weight_decay=1e-4):\n",
    "    logits = W[[x for x in xs.T]]\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = average negative log likelihood\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean() + weight_decay*(W**2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the \"model\"\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(tuple(len(vocab) for _ in range(n)), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(600):\n",
    "    # forward pass\n",
    "    tr_loss = calc_loss(xs, ys, W, 0)\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    tr_loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -100 * W.grad\n",
    "\n",
    "    if ep % 10 == 9:\n",
    "        print(f'{ep+1:>3}th epoch, tr_loss={tr_loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junjdedianaqidouxutnypaxnuq\n",
      "jimrltozsogjatqzvugignaduwjbuldvhajzdbiminrwimpadsvzywcfxvbryn\n",
      "farmumtkyf\n",
      "demmerponnsleigh\n",
      "ani\n",
      "cora\n",
      "yaehocpkqjyked\n",
      "webdmeiibwyaftwtiansnhspoluwaspphfdgosfmxtpqcixz\n",
      "repahfmtydt\n",
      "jayrslu\n",
      "isa\n",
      "dyfj\n",
      "mjluuj\n",
      "mahvupwyilpvhecgiagr\n",
      "jenhwvdxtta\n",
      "malyn\n",
      "brey\n",
      "aui\n",
      "lavlpocq\n",
      "themilana\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "n_samples = 20\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(n_samples):\n",
    "    ixs = deque([stoi[SEP_TOK]] * (n-1))\n",
    "    out = []\n",
    "    while True:\n",
    "        logits = W[tuple(ixs)]\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum()\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == stoi[SEP_TOK]:\n",
    "            break\n",
    "        ixs.popleft()\n",
    "        ixs.append(ix)\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
